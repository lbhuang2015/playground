oyzKIkLYbGdMX8Qxezla4YkrEMGOslk14Tz
//@version=6
strategy("SPY RTH - Dynamic Bias Master v3", overlay=true, initial_capital=10000, default_qty_type=strategy.percent_of_equity, default_qty_value=100)

// --- 1. æ ¸å¿ƒå‚æ•°è®¾ç½® ---
lookback_mins   = input.int(15, "å¼€ç›˜å¦å®šçª—å£ (min)")
rejection_ratio = input.float(0.5, "å¦å®šæ·±åº¦é˜ˆå€¼")
vol_mult        = input.float(1.2, "æˆäº¤é‡æ”¾å¤§å€æ•°")
sw_len          = input.int(10, "äºŒæ³¢æ­¢æŸå‚è€ƒKçº¿æ•°")
max_daily_loss  = input.float(1.5, "æ—¥å†…æœ€å¤§å›æ’¤ä¿æŠ¤ (%)")

// --- 2. æ–°å¢ï¼šæ—¥å†…å¼€ä»“æ¬¡æ•°å¯è°ƒå‚æ•° ---
max_trades_day  = input.int(3, "æ—¥å†…æœ€å¤§äº¤æ˜“æ¬¡æ•°", minval=1, maxval=5)

// --- 3. åˆ†æ‰¹å¹³ä»“ä¸åˆ©æ¶¦ä¿æŠ¤ ---
tp1_target_pct  = input.float(0.3, "ç¬¬ä¸€æ­¢ç›ˆç›®æ ‡ (%)")
scaling_qty_pct = input.int(50, "åˆ†æ‰¹å¹³ä»“æ¯”ä¾‹ (%)")
be_activation   = input.float(0.2, "ä¿æœ¬è§¦å‘é˜ˆå€¼ (%)")
callback_pct    = input.float(0.12, "åˆ©æ¶¦å›æ’¤å¹³ä»“æ¯”ä¾‹ (%)")
exit_ema_len    = input.int(20, "å®ä½“æ­¢ç›ˆå‡çº¿ (EMA9)")

// --- 4. å…¨å±€è®¡ç®— ---
vwap_val      = ta.vwap(close)
exit_ema      = ta.ema(close, exit_ema_len)
vol_ma        = ta.sma(volume, 20)

is_crossover_vwap  = ta.crossover(close, vwap_val)
is_crossunder_vwap = ta.crossunder(close, vwap_val)

// å®ä½“æ­¢ç›ˆåˆ¤å®š
body_exit_call = open < exit_ema and close < exit_ema
body_exit_put  = open > exit_ema and close > exit_ema

is_rth = time(timeframe.period, "0930-1600:23456", "America/New_York") != 0
rth_start_bar = is_rth and not is_rth[1]
bars_since_open = ta.barssince(rth_start_bar)

// --- 5. åŠ¨æ€åè§é€»è¾‘ä¸å¼€ç›˜ä»·è®°å½• ---
var bool bias_l = false
var bool bias_s = false
var float prev_c = na
var float eth_h = na
var float eth_l = na
var float day_open = na 
var bool tp1_hit = false

if ta.change(time("D", "America/New_York")) != 0
    prev_c := close[1], eth_h := high, eth_l := low, bias_l := false, bias_s := false, tp1_hit := false
    day_open := open 
else if not is_rth
    eth_h := math.max(eth_h, high), eth_l := math.min(eth_l, low)

in_window = is_rth and bars_since_open <= (lookback_mins / timeframe.multiplier)
long_1st  = in_window and (prev_c - eth_l) > 0 and close > (eth_l + (prev_c - eth_l) * rejection_ratio) and volume > vol_ma * vol_mult
short_1st = in_window and (eth_h - prev_c) > 0 and close < (eth_h - (eth_h - prev_c) * rejection_ratio) and volume > vol_ma * vol_mult

if long_1st
    bias_l := true
if short_1st
    bias_s := true

// åè§ç¿»è½¬ (Bias Flip) - è§£å†³12æœˆ31æ—¥ç±»ä¼¼åè½¬è¡Œæƒ…
if bias_l and is_crossunder_vwap and not in_window
    bias_l := false
    bias_s := true
if bias_s and is_crossover_vwap and not in_window
    bias_s := false
    bias_l := true

// --- 6. äº¤æ˜“æ‰§è¡Œ (åº”ç”¨ max_trades_day å‚æ•°) ---
var int trades_count = 0
if ta.change(time("D", "America/New_York")) != 0
    trades_count := 0
if strategy.position_size[1] == 0 and strategy.position_size != 0
    trades_count := trades_count + 1

time_filter = hour < 14 or (hour == 14 and minute < 30)

// åè½¬å…¥åœºå¢åŠ å¼€ç›˜ä»·è¿‡æ»¤
long_2nd  = bias_l and strategy.position_size == 0 and is_crossover_vwap and close > day_open and is_rth and not in_window and time_filter
short_2nd = bias_s and strategy.position_size == 0 and is_crossunder_vwap and close < day_open and is_rth and not in_window and time_filter

// åˆå§‹åŒ–ä¸º na è§£å†³å›¾è¡¨ç¼©æ”¾é—®é¢˜
var float active_sl = na
var float hi_seen = na
var float lo_seen = na

lowest_sw = ta.lowest(low, sw_len)
highest_sw = ta.highest(high, sw_len)

if (trades_count < max_trades_day) and strategy.position_size == 0
    if (long_1st or long_2nd)
        strategy.entry("Long", strategy.long, comment=long_1st ? "W1" : "W2_Flip")
        active_sl := long_1st ? low : math.min(lowest_sw, vwap_val - 0.5)
        tp1_hit := false
    else if (short_1st or short_2nd)
        strategy.entry("Short", strategy.short, comment=short_1st ? "W1" : "W2_Flip")
        active_sl := short_1st ? high : math.max(highest_sw, vwap_val + 0.5)
        tp1_hit := false

// --- 7. æŒä»“è¿½è¸ªä¸ç¦»åœº ---
if strategy.position_size != 0
    if strategy.position_size > 0
        hi_seen := na(hi_seen) ? high : math.max(hi_seen, high)
        if not tp1_hit and hi_seen >= strategy.position_avg_price * (1 + tp1_target_pct/100)
            tp1_hit := true
            strategy.close("Long", qty_percent=scaling_qty_pct, comment="TP1_Half")
        if hi_seen >= strategy.position_avg_price * (1 + be_activation/100)
            active_sl := math.max(active_sl, strategy.position_avg_price + syminfo.mintick * 10)
    else
        lo_seen := na(lo_seen) ? low : math.min(lo_seen, low)
        if not tp1_hit and lo_seen <= strategy.position_avg_price * (1 - tp1_target_pct/100)
            tp1_hit := true
            strategy.close("Short", qty_percent=scaling_qty_pct, comment="TP1_Half")
        if lo_seen <= strategy.position_avg_price * (1 - be_activation/100)
            active_sl := math.min(active_sl, strategy.position_avg_price - syminfo.mintick * 10)
else
    active_sl := na, hi_seen := na, lo_seen := na // å½»åº•é˜²æ­¢ K çº¿è¢«æŒ¤å‹

trailing_exit = tp1_hit and (strategy.position_size > 0 ? close < hi_seen * (1 - callback_pct / 100) : close > lo_seen * (1 + callback_pct / 100))

if strategy.position_size != 0
    if (strategy.position_size > 0 and close < active_sl) or (strategy.position_size < 0 and close > active_sl)
        strategy.close_all(comment="SL/BE")
    else if (strategy.position_size > 0 and body_exit_call) or (strategy.position_size < 0 and body_exit_put) or trailing_exit
        strategy.close_all(comment="Dynamic_Exit")

if (ta.crossunder(time, timestamp("America/New_York", year, month, dayofmonth, 15, 50)))
    strategy.close_all(comment="EOD")

// --- 8. ç»˜å›¾ ---
plot(strategy.position_size != 0 ? active_sl : na, "SL/BE_Line", color=color.new(color.red, 40), style=plot.style_linebr, linewidth=2)
plot(vwap_val, "VWAP", color=color.new(color.orange, 50), linewidth=2)
plot(exit_ema, "EMA20_Exit", color=color.new(color.blue, 60))



import requests
import json
import sys

def run_test():
    url = "http://127.0.0.1:11434/v1/chat/completions"
    payload = {
        "model": "gemma3:27b",
        "messages": [{"role": "user", "content": "Ping!"}],
        "stream": False
    }

    print("--- æ­£åœ¨å‘èµ·è¿æ¥æµ‹è¯• ---")
    try:
        # å‘èµ·è¯·æ±‚
        response = requests.post(url, json=payload, timeout=60)
        
        # æ£€æŸ¥ HTTP çŠ¶æ€ç 
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            print("âœ… æˆåŠŸè¿é€š Ollamaï¼")
            print(f"æ¨¡å‹å›å¤: {content}")
        else:
            print(f"âŒ æœåŠ¡å™¨è¿”å›äº†é”™è¯¯ç : {response.status_code}")
            print(f"é”™è¯¯è¯¦æƒ…: {response.text}")

    except requests.exceptions.ConnectionError:
        print("ğŸš¨ é”™è¯¯: æ— æ³•è¿æ¥åˆ° Ollamaã€‚è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œä¸”æ²¡æœ‰è¢«é˜²ç«å¢™æ‹¦æˆªã€‚")
    except Exception as e:
        print(f"ğŸš¨ å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__}: {e}")

# ã€å…³é”®ã€‘ç¡®ä¿è„šæœ¬è¢«è¿è¡Œæ—¶ä¼šç›´æ¥æ‰§è¡Œå‡½æ•°
if __name__ == "__main__":
    run_test()
    input("\næŒ‰å›è½¦é”®é€€å‡º...") # é˜²æ­¢çª—å£é—ªé€€ï¼Œè®©ä½ çœ‹æ¸…ç»“æœ


**Target:** Refactor the Log Analysis Tool to generate a "Compact Performance Profile" and disable fragmenting/chunking.

**Instructions:**

Modify the Spark log parsing tool to output a highly condensed, single-document summary of skewed stages. The goal is to keep the total output under **8,000 characters** to ensure the LLM can process the entire job context in a single inference pass.

**Requirements:**

1. **Strict Context Consolidation:**
   - **Disable all chunking/fragmenting logic.** Do not split the output into 4000-character segments. The LLM must receive all skewed stages in one message to perform comparative analysis.
2. **Aggressive Data Condensation:**
   - **Use Abbreviated Headers:** Rename metrics to save space (e.g., `Duration` -> `Dur`, `Input Size` -> `In`, `Shuffle Read` -> `ShRead`, `Shuffle Write` -> `ShWrite`).
   - **Statistical Distribution Only:** For "Summary Metrics for Completed Tasks," only output five key data points: **[Min, P25, P50, P75, Max]**. Do not include task counts or individual task IDs.
   - **Executor Outlier Summary:** Instead of listing all executors, only list the **Top 3 Slowest** and **Top 2 Fastest** executors for each outlier stage. This highlights the gap without bloating the text.
3. **Refined Filtering:**
   - Only process stages where $Max(Duration) / Median(Duration) > 3$.
   - If multiple stages are skewed, sort them by **Total Duration** (descending) so the most impactful bottleneck appears first.
4. **Formatting for LLM Efficiency:**
   - Use a **compact Markdown Table** format.
   - Use scientific notation or simplified units (e.g., `1.5GB` instead of `1,610,612,736 bytes`).
   - Round all durations to **1 decimal place** (e.g., `12.4s`).

**Example of Desired Compact Output:**

```
| Stage | Tasks | Dur(Min/P50/Max) | In | ShRead | SkewRatio |
|-------|-------|------------------|----|--------|-----------|
| 12    | 2000  | 0.1s/1.2s/45.0m  | 8G | 2G     | 2250.0x   |
```

**Goal:** Produce a single, comprehensive "Performance Snapshot" that fits within the LLM's high-speed attention window (approx. 8k-10k chars) to enable a definitive diagnosis in under 60 seconds on a CPU-bound environment.




Copilot ä»»åŠ¡ï¼šé‡æ„ Spark è¯Šæ–­å·¥å…·ä¸ Prompt é€»è¾‘
Context: We are analyzing 1.4GB Spark logs on a CPU-bound, restricted machine. We must avoid multi-segment processing and GPU dependency.

Task 1: Refactor the Tool (Python Logic) Modify the log-parsing tool to extract metrics and output a single, condensed "Essence Pack".

Streaming Parse: Scan the log line-by-line. Extract metrics for Stages with Skew Ratio > 3.

Abbreviate & Compact: Use headers like Dur, Skew, Shuf, Spill. Round all floats to 1 decimal.

Aggregation: Instead of raw task lists, provide [Min, P50, P75, Max] distributions and a "Top 3 Outlier Executors" list.

No Chunking: Disable the 4000-char limit. Output the entire result (targeted < 8k chars) as a single block.

Task 2: Inject the System Prompt (LLM Reasoning) Set the following as the System Instructions for the LLM when it receives the extracted metrics:

Markdown
# Role
You are a Senior Spark Performance Engineer specializing in large-scale financial data processing.

# Task
Analyze the provided Spark metrics and generate a definitive Diagnostic Report.

# Response Template (MUST follow this structure):

## Part 1: Performance Metrics Table
| StageID | SkewRatio | Duration | Tasks (S/F/T) | In/Out | Shuf R/W | Spill (M/D) | GC Time |
|:---|:---|:---|:---|:---|:---|:---|:---|
| {id} | {max/med} | {total} | {succ/fail/tot} | {size} | {size} | {mem/disk} | {time} |

## Part 2: Core Root Cause Analysis
- Identify the primary bottleneck (e.g., Data Skew, Resource Starvation, or Small File Problem).
- Explain *why* the metrics point to this conclusion (e.g., "Stage 10 has a 200x SkewRatio with high Disk Spill, indicating a skewed Join Key").

## Part 3: Actionable Optimization Recommendations
- **Targeted Stages**: Specifically mention which IDs to fix.
- **Spark Configs**: Provide specific parameters (e.g., `spark.sql.shuffle.partitions`, `spark.memory.fraction`).
- **Code Strategy**: Suggest Salting, Broadcast Joins, or Repartitioning where applicable.
Task 3: Execution Constraint Ensure the Python tool feeds the extracted data directly into the {metrics_data} slot of the prompt below:

"Below are the condensed metrics for a Spark Job. Based on these outliers, generate the three-part Diagnostic Report defined in your instructions: {metrics_data}"


### Prompt 1: MCP Tool Selection & Extraction Logic

**Goal**: Instruct the MCP tool (or an underlying script) to stream-process massive logs and extract only the "Performance DNA" based on your specific thresholds.

> **Task**: Spark Log Distiller (Application Performance Context Extractor)
>
> **Objective**: Stream-process raw Spark EventLogs and DriverLogs for a specific `application_id` to generate a high-density performance context. The output must mimic the Spark UIâ€™s depth but filter out noise based on specific performance heuristics.
>
> **1. Extraction & Pre-calculation Requirements**:
>
> - **Environment**: Extract all `Spark Properties` from `SparkListenerEnvironmentUpdate`. Exclude paths, classpaths, and sensitive credentials.
>
> - **Job Summaries**: Reconstruct the Job List (matching Spark UI) by correlating `SparkListenerJobStart` and `SparkListenerJobEnd`. Calculate total duration and identify associated Stage IDs.
>
> - **Stage List & Descriptions**: Reconstruct the Stage List. Crucially, extract the `Description` from the `callSite` or `spark.job.description` property to map stages to specific Java/Scala source files and line numbers.
>
> - **Stage Detailed Metrics**: For each stage, aggregate metrics from all `SparkListenerTaskEnd` events to reconstruct the "Summary Metrics" table (Min, 25th, Median, 75th, Max). Include: *Duration, GC Time, Input Size, Shuffle Read/Write, and Disk/Memory Spill*.
>
> - **Skew Detection**: Calculate the **DataSkew Ratio** for every stage:
>
>   $$Ratio = \frac{Max\_Task\_Duration}{Median\_Task\_Duration} \times 100\%$$
>
> **2. Filtering & Pruning Logic (Crucial)**:
>
> Only include data in the final output context if it meets at least one of these criteria:
>
> - **Failures**: Any Job/Stage with an `Error` or `Exception` status, including the full stack trace from DriverLogs.
> - **Performance Hotspots**:
>   1. The Job total duration is **> 5 minutes**.
>   2. **AND** the Stage within that job has a **DataSkew Ratio > 130%**.
>
> **3. Output Structure**:
>
> Return a structured Markdown report containing the [Environment] summary, a list of [Critical Exceptions], and [Deep-Dive Metrics] for filtered Jobs/Stages, including the corresponding SQL Physical Plan snippet.

------

### Prompt 2: LLM Performance Expert System Prompt

**Goal**: The "Brain" that receives the filtered logs and provides the root-cause analysis and code-level fixes.

> **Role**: Senior Spark Performance & Tuning Engineer
>
> **Context**: You will receive a distilled performance report containing Spark environment settings, execution metrics, and SQL physical plans. This data has been pre-filtered to focus on jobs exceeding 5 minutes with significant data skew (>130%) or execution errors.
>
> **Your Mission**: Perform a forensic performance analysis and provide a structured "Optimization Roadmap."
>
> **Analysis Requirements**:
>
> 1. **Code-Level Mapping**: Use the `Description/CallSite` field to identify the exact line of code (File:Line) causing the bottleneck. Explain what the transformation (e.g., Join, Window, Aggregate) is doing at the physical level.
> 2. **Bottleneck Classification**:
>    - **Data Skew**: If Ratio > 130%, identify the skewed Join/Aggregation key in the Physical Plan. Suggest `Salting` strategies or `AQE Skew Join` configurations.
>    - **Resource Exhaustion**: Check for `Disk Spilling` or high `GC Time`. Recommend specific memory fraction adjustments or executor core changes.
>    - **Plan Regression**: Identify inefficient plans, such as `SortMergeJoin` where `BroadcastHashJoin` should have occurred.
> 3. **Actionable Recommendations**:
>    - **Configuration**: Provide specific key-value pairs (e.g., `set spark.sql.shuffle.partitions=1000`).
>    - **Code**: Provide Scala/Java/Python pseudo-code snippets for logic refactoring.
>
> **Output Format**:
>
> ## [Executive Summary]
>
> Brief overview of the application health and the "Top 1" bottleneck.
>
> ## [Stage-Level Deep Dive: Stage {ID}]
>
> - **Source Location**: `{file_name}:{line_number}`
> - **Issue**: (e.g., Heavy Data Skew / Executor OOM / Spill)
> - **Evidence**: (Cite specific Task Metrics: Max vs. Median)
> - **Plan Analysis**: (Point out the bottleneck node in the Physical Plan)
>
> ## [Prescribed Fixes]
>
> - **Immediate Configuration Changes**: [List]
> - **Code Refactoring Plan**: [Detailed advice]
>
> **Tone**: Technical, data-driven, and concise. Do not speculate; base all conclusions on the provided metrics and logs.




